<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/all.min.css" integrity="sha512-xh6O/CkQoPOWDdYTDqeRdPCVd1SpvCA9XXcUnZS2FmJNp1coAFzvtCN9BmamE+4aHK8yyUHUSCcJHgXloTyT2A==" crossorigin="anonymous" referrerpolicy="no-referrer" />

  <link rel="stylesheet" href="./assets/home/css/home.css">

  <style>
    /* 전체 볼드 톤 유지 */
    b, strong, h1, h2, h3, h4, h5, h6 { font-weight: 600; }

    /* 소개 단락 스타일 */
    .profile-text { font-size: 1rem; line-height: 1.7; font-weight: 400; }

    /* Publications 스타일 */
    .pub-title { font-size: 1.02rem; font-weight: 600; line-height: 1.5; }
    .pub-meta  { font-size: 0.9rem; color: #555; }
    .pub-desc  { font-size: 0.95rem; color: #444; margin-top: 6px; line-height: 1.5; font-weight: 400; }
    .pub-links { font-size: 0.9rem; margin-top: 4px; }
    .pub + .pub { margin-top: 1.8rem; } 
    .badge-light { font-size: 0.8rem; color: #666; background-color: #f8f9fa; border: 1px solid #ddd; margin-top: 4px; }
    .badge-warning { font-size: 0.8rem; margin-top: 4px; } /* Best Paper 배지용 */

    .section { margin-top: 2.5rem; font-weight: 600; border-bottom: 1px solid #eee; padding-bottom: 10px; margin-bottom: 15px;}
    
    /* 리스트 스타일 조정 */
    ul.experience-list li { margin-bottom: 10px; }
    .date-badge { float: right; font-size: 0.85rem; color: #666; font-weight: 400; }
  </style>

  <script async src="https://www.googletagmanager.com/gtag/js?id=G-CMSVGPC040"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-CMSVGPC040');
  </script>

  <title>Seungwoo Son</title>
  <link rel="icon" type="image/x-icon" href="assets/favicon.ico">
</head>

<body>
  <div class="container">
    <div class="row">
      <div class="col-lg-1"></div>
      <div class="col-lg-10">
        <div class="row" style="margin-top: 4em;">
          <div class="col-md-4 d-flex justify-content-start flex-column" align="center">
            <div style="margin-bottom: 20px;">
              <img src="./assets/home/image/profile.jpg" class="rounded-circle" width="200px" alt="Seungwoo Son" onerror="this.src='https://via.placeholder.com/200?text=Profile+Pic'">
            </div> 
            <div>
              <a href="mailto:swson32@gmail.com" target="_blank"><i class="fas fa-envelope"></i> Email</a>
              |
              <a href="https://scholar.google.com/citations?user=37LYQu4AAAAJ&hl=en" target="_blank">Scholar</a>
              |
              <a href="https://www.linkedin.com/in/seungwoo-son-057511323" target="_blank">LinkedIn</a>
            </div>
          </div>

          <div class="col-md-8">
            <h3 style="margin: 0; font-weight: 600;">Seungwoo Son</h3>
            <small class="text-muted"> Machine Learning Engineer @ Samsung Research </small>

            <p class="profile-text text-justify mt-3">
              I am a Machine Learning Engineer at <strong>Samsung Research</strong> (AI System Team). 
              Previously, I worked at <strong>Google</strong> (CoreML Team) as a Student Researcher Intern.
            </p>

            <p class="profile-text text-justify mb-0">
              My research interests lie in <strong>Model Compression</strong> and <strong>On-device Personalization</strong>.
              Recently, I have been exploring ways to internalize retrieval augmented generation (e.g., GraphRAG) on edge devices for personalized AI.
              I have experience developing quantization methods that significantly reduce model size and latency while maintaining accuracy.
            </p>
          </div>
        </div>

        <div class="row" style="margin-top: 1em;">
          <div class="col-sm-12" style="padding: 0;">
            
            <h4 class="section">Work Experience</h4>
            <ul class="experience-list">
              <li>
                  <strong>Machine Learning Engineer</strong>, Samsung Research (AI System Team)
                  <span class="date-badge">Oct. 2024 - Present</span>
                  <br><small class="text-muted">Developing quantized models for Galaxy edge devices. Reduced model size by 75% and latency by 30%.</small>
              </li>
              <li>
                  <strong>Student Researcher Intern</strong>, Google (CoreML Team)
                  <span class="date-badge">Aug. 2023 - Jul. 2024</span>
                  <br><small class="text-muted">Implemented advanced quantization methods for LLMs, achieving 50% improvement in zero-shot accuracy.</small>
              </li>
              <li>
                  <strong>Graduate Research Assistant</strong>, POSTECH
                  <span class="date-badge">Mar. 2022 - Jul. 2024</span>
                  <br><small class="text-muted">Researched neural network compression techniques (KD, Quantization).</small>
              </li>
            </ul>

            <h4 class="section">Education</h4>
            <ul class="experience-list">
                <li>
                    <strong>Pohang University of Science and Technology (POSTECH)</strong>
                    <span class="date-badge">Mar. 2022 - Aug. 2024</span>
                    <br>M.S. in Electrical Engineering
                </li>
                <li>
                    <strong>Inha University</strong>
                    <span class="date-badge">Mar. 2016 - Feb. 2022</span>
                    <br>B.S. in Electronic Engineering (Total GPA: 4.33/4.5, Major GPA: 4.4/4.5, Summa Cum Laude)
                </li>
            </ul>

            <h4 class="mt-2 section">Publications</h4>

            <div class="row pub">
              <div class="col-sm-12 pub-text">
                <div class="pub-title">TurboBoA: Faster and Exact Attention-aware Quantization without Backpropagation</div>
                <div class="pub-meta">Junhan Kim, Yeo Jeong Park, <strong>Seungwoo Son</strong>, Chungman Lee, Ho-young Kim, Joonyoung Kim, Yongkweon Jeon &nbsp;|&nbsp; <span class="pub-venue">ICLR 2026</span> (Submitted)</div>
                <div class="pub-desc">
                  Proposed a backpropagation-free quantization algorithm that achieves 4x speedup over state-of-the-art methods by jointly quantizing multiple out channels and correcting propagated distortions, delivering superior accuracy in low-bit regimes.
                </div>
                <div><span class="badge badge-light">Work done at Samsung Research</span></div>
              </div>
            </div>

            <div class="row pub">
              <div class="col-sm-12 pub-text">
                <div class="pub-title">On the Importance of a Multi-Scale Calibration for Quantization</div>
                <div class="pub-meta"><strong>Seungwoo Son</strong>, Junhan Kim, Ingyu Seong, Hyemi Jang, Yongkweon Jeon &nbsp;|&nbsp; <span class="pub-venue">ICASSP 2026</span> (Submitted)</div>
                <div class="pub-desc">
                  Introduced MaCa, a length-aware calibration method that incorporates multi-scale sequence length information into Hessian estimation to improve quantization accuracy for variable-length inputs in LLMs.
                </div>
                <div><span class="badge badge-light">Work done at Samsung Research</span></div>
              </div> 
            </div> 

            <div class="row pub">
              <div class="col-sm-12 pub-text">
                <div class="pub-title">Two-Stage Grid Optimization for Group-wise Quantization of LLMs</div>
                <div class="pub-meta">Junhan Kim, <strong>Seungwoo Son</strong>, Jeewook Kim, Gukryeol Lee, Yongkweon Jeon &nbsp;|&nbsp; <span class="pub-venue">ICASSP 2026</span> (Submitted)</div>
                <div class="pub-desc">
                  Developed a two-stage optimization strategy for group-wise quantization that initializes group scales based on input statistics and refines them via closed-form coordinate descent, minimizing layer-wise reconstruction loss efficiently.
                </div>
                <div><span class="badge badge-light">Work done at Samsung Research</span></div>
              </div> 
            </div> 

            <div class="row pub">
              <div class="col-sm-12 pub-text">
                <div class="pub-title">Prefixing Attention Sinks can Mitigate Activation Outliers for Large Language Model Quantization</div>
                <div class="pub-meta"><strong>Seungwoo Son</strong>, Wonpyo Park, Woohyun Han, Kyuyeun Kim, Jaeho Lee &nbsp;|&nbsp; <span class="pub-venue">EMNLP 2024</span></div>
                <div class="pub-desc">
                  Revealed that prepending attention sink tokens mitigates activation outliers in LLMs by absorbing massive attention scores, enabling effective activation quantization.
                </div>
                <div class="pub-links">
                  [<a href="https://aclanthology.org/2024.emnlp-main.134/" target="_blank">Paper</a>]
                  <span class="badge badge-light ml-1">Work done at Google</span>
                </div>
              </div> 
            </div> 

            <div class="row pub">
              <div class="col-sm-12 pub-text">
                <div class="pub-title">The Role of Masking for Efficient Supervised Knowledge Distillation of Vision Transformers</div>
                <div class="pub-meta">
                  <strong>Seungwoo Son</strong>, Jegwang Ryu, Namhoon Lee, Jaeho Lee &nbsp;|&nbsp; 
                  <span class="pub-venue">ECCV 2024</span> | <span class="pub-venue">ICLR 2023 Workshop on Sparsity in Neural Networks</span>
                </div>
                <div class="pub-desc">
                  Developed a cost-efficient distillation framework for Vision Transformers by masking input tokens to the teacher.
                </div>
                <div class="pub-links">[<a href="https://arxiv.org/abs/2302.10494v1" target="_blank">Paper</a>] [<a href="https://github.com/effl-lab/MaskedKD" target="_blank">Code</a>]</div>
              </div> 
            </div> 

            <div class="row pub">
              <div class="col-sm-12 pub-text">
                <div class="pub-title">DSP: Distill The Knowledge Only By A Subset of Patches</div>
                <div class="pub-meta">
                  <strong>Seungwoo Son</strong> et al. &nbsp;|&nbsp; 
                  <span class="pub-venue">IPIU 2023</span> (Oral)
                </div>
                <div class="pub-desc">
                  Investigated methodology to efficiently extract model knowledge using only a subset of image patches.
                </div>
                <div class="pub-links">
                  [<a href="http://ipiu2023.ipiu.or.kr/?act=info.page&pcode=sub04" target="_blank">Link</a>]
                  <span class="badge badge-warning ml-1"><i class="fas fa-trophy"></i> Best Paper Award</span>
                </div>
              </div> 
            </div> 


            <h4 class="section">Invited Talks</h4>
            <ul class="experience-list">
              <li>
                <strong>Naver-Intel Joint Lab Workshop: Lightweighting for Hyperscale AI</strong>
                <span class="date-badge">Jun. 2024</span>
                <br>
                <small class="text-muted"><a href="https://www.kiise.or.kr/conference/main/getContent.do?CC=KCC&CS=2024&PARENT_ID=011700&content_no=2013" target="_blank">Conference Info</a></small>
              </li>
            </ul>

            <h4 class="section">Academic Services</h4>
            <ul>
              <li><strong>Reviewer:</strong> ACL 2026, EACL 2026, ACL 2025, EMNLP 2025</li>
            </ul>

            <h4 class="section">Honors & Awards</h4>
            <ul>
              <li><strong>Best M.S. Dissertation Award</strong>, POSTECH (Feb. 2025)</li>
              <li><strong>IPIU Best Paper Award</strong>, Korea Computer Vision Society (Feb. 2023)</li>
              <li><strong>National Science and Engineering Undergraduate Scholarship</strong>, Ministry of Science and ICT (Mar. 2020)</li>
            </ul>

            <h4 class="section">Technical Skills</h4>
            <ul>
                <li><strong>Languages:</strong> C/C++, Python</li>
                <li><strong>Frameworks & Tools:</strong> PyTorch, Jax, Git, Overleaf</li>
            </ul>

          </div>
        </div>
      </div>
      <div class="col-lg-1"></div>
    </div>

    <footer class="footer mt-5 mb-2 border-top">
      <p class="text-center">
        <small>© 2025 Seungwoo Son.</small>
      </p>
    </footer>
  </div>

  <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha384-KJ3o2DKtIkv3YIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN"
    crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js"
    integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q"
    crossorigin="anonymous"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js"
    integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl"
    crossorigin="anonymous"></script>
</body>
</html>
